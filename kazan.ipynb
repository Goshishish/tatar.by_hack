{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\работа\\прога\\ai\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, TimeDistributed, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Представим, что у нас есть набор данных с предложениями\n",
    "english_sentences = pd.read_csv(\"translations.csv\")['ru'][:10]\n",
    "tatar_sentences = pd.read_csv(\"translations.csv\")['tat'][:10]\n",
    "\n",
    "# Токенизация текста\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_tat = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(english_sentences)\n",
    "tokenizer_tat.fit_on_texts(tatar_sentences)\n",
    "\n",
    "# Преобразование текста в последовательности чисел\n",
    "sequences_eng = tokenizer_eng.texts_to_sequences(english_sentences)\n",
    "sequences_tat = tokenizer_tat.texts_to_sequences(tatar_sentences)\n",
    "\n",
    "# Паддинг последовательностей для выравнивания длины\n",
    "max_len_eng = max(len(x) for x in sequences_eng)\n",
    "max_len_tat = max(len(x) for x in sequences_tat)\n",
    "max_len = max(max_len_eng, max_len_tat)  # Выровнять длину последовательностей по максимальному значению\n",
    "\n",
    "padded_eng = pad_sequences(sequences_eng, maxlen=max_len, padding='post')\n",
    "padded_tat = pad_sequences(sequences_tat, maxlen=max_len, padding='post')\n",
    "\n",
    "# Параметры модели\n",
    "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
    "vocab_size_tat = len(tokenizer_tat.word_index) + 1\n",
    "embedding_dim = 128\n",
    "units = 128\n",
    "\n",
    "# Построение модели\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size_eng, embedding_dim, input_length=max_len),\n",
    "    Bidirectional(LSTM(units, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(units, return_sequences=True)),\n",
    "    TimeDistributed(Dense(vocab_size_tat, activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 521ms/step - accuracy: 0.3134 - loss: 2.5892 - val_accuracy: 0.8857 - val_loss: 0.7649\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4687 - loss: 2.1461 - val_accuracy: 0.8351 - val_loss: 0.8194\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2796 - loss: 2.4542 - val_accuracy: 0.8478 - val_loss: 0.7940\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3933 - loss: 2.0091 - val_accuracy: 0.8351 - val_loss: 0.8173\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.3057 - loss: 2.0969 - val_accuracy: 0.7845 - val_loss: 0.8956\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2814 - loss: 2.1083 - val_accuracy: 0.8224 - val_loss: 0.8347\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2786 - loss: 2.2458 - val_accuracy: 0.7971 - val_loss: 0.8749\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2666 - loss: 1.9208 - val_accuracy: 0.7592 - val_loss: 0.9342\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4369 - loss: 1.4077 - val_accuracy: 0.7971 - val_loss: 0.8928\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2709 - loss: 1.8506 - val_accuracy: 0.7845 - val_loss: 0.9314\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3103 - loss: 1.6268 - val_accuracy: 0.7592 - val_loss: 0.9994\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3043 - loss: 1.5315 - val_accuracy: 0.7592 - val_loss: 0.9715\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4376 - loss: 1.2149 - val_accuracy: 0.7592 - val_loss: 0.9995\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3152 - loss: 1.4159 - val_accuracy: 0.7592 - val_loss: 0.9758\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3852 - loss: 1.2698 - val_accuracy: 0.7592 - val_loss: 1.0037\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2933 - loss: 1.3867 - val_accuracy: 0.7465 - val_loss: 1.1371\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2386 - loss: 1.4895 - val_accuracy: 0.7592 - val_loss: 1.0801\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4164 - loss: 1.0606 - val_accuracy: 0.7592 - val_loss: 1.0011\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.3185 - loss: 1.2063 - val_accuracy: 0.7592 - val_loss: 1.0700\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3868 - loss: 1.0518 - val_accuracy: 0.7592 - val_loss: 1.1103\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.3972 - loss: 0.9851 - val_accuracy: 0.7592 - val_loss: 1.1137\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3694 - loss: 0.9748 - val_accuracy: 0.7592 - val_loss: 1.1008\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4061 - loss: 0.9254 - val_accuracy: 0.7592 - val_loss: 1.1024\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2298 - loss: 1.2431 - val_accuracy: 0.7592 - val_loss: 1.1465\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2557 - loss: 1.0873 - val_accuracy: 0.7592 - val_loss: 1.1788\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2848 - loss: 0.9764 - val_accuracy: 0.7592 - val_loss: 1.1279\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2663 - loss: 1.0903 - val_accuracy: 0.7592 - val_loss: 1.1690\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3926 - loss: 0.7853 - val_accuracy: 0.7592 - val_loss: 1.1694\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3259 - loss: 0.8735 - val_accuracy: 0.7592 - val_loss: 1.2206\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3729 - loss: 0.7734 - val_accuracy: 0.7592 - val_loss: 1.2186\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2889 - loss: 0.8740 - val_accuracy: 0.7592 - val_loss: 1.1921\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4062 - loss: 0.7366 - val_accuracy: 0.7592 - val_loss: 1.2081\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2800 - loss: 0.9240 - val_accuracy: 0.7592 - val_loss: 1.2672\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4574 - loss: 0.6410 - val_accuracy: 0.7592 - val_loss: 1.2455\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2688 - loss: 0.9298 - val_accuracy: 0.7592 - val_loss: 1.2368\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2550 - loss: 0.9247 - val_accuracy: 0.7592 - val_loss: 1.2432\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2929 - loss: 0.7829 - val_accuracy: 0.7465 - val_loss: 1.3342\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2220 - loss: 0.9147 - val_accuracy: 0.7592 - val_loss: 1.2403\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2354 - loss: 1.0043 - val_accuracy: 0.7465 - val_loss: 1.3247\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2460 - loss: 0.8691 - val_accuracy: 0.7592 - val_loss: 1.2821\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3888 - loss: 0.6439 - val_accuracy: 0.7592 - val_loss: 1.2461\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3514 - loss: 0.6904 - val_accuracy: 0.7465 - val_loss: 1.3569\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2485 - loss: 0.8903 - val_accuracy: 0.7592 - val_loss: 1.2838\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3984 - loss: 0.5804 - val_accuracy: 0.7592 - val_loss: 1.2706\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3277 - loss: 0.6313 - val_accuracy: 0.7592 - val_loss: 1.3345\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2737 - loss: 0.6891 - val_accuracy: 0.7592 - val_loss: 1.3312\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.3580 - loss: 0.5615 - val_accuracy: 0.7592 - val_loss: 1.3242\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.2226 - loss: 0.7195 - val_accuracy: 0.7465 - val_loss: 1.3704\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4015 - loss: 0.5286 - val_accuracy: 0.7592 - val_loss: 1.3249\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2577 - loss: 0.6539 - val_accuracy: 0.7592 - val_loss: 1.3052\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3483 - loss: 0.5562 - val_accuracy: 0.7465 - val_loss: 1.3632\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3218 - loss: 0.5116 - val_accuracy: 0.7592 - val_loss: 1.3372\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2651 - loss: 0.5926 - val_accuracy: 0.7592 - val_loss: 1.3331\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2889 - loss: 0.5855 - val_accuracy: 0.7465 - val_loss: 1.4026\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3519 - loss: 0.5061 - val_accuracy: 0.7465 - val_loss: 1.4161\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2426 - loss: 0.5929 - val_accuracy: 0.7592 - val_loss: 1.3743\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3955 - loss: 0.4076 - val_accuracy: 0.7592 - val_loss: 1.3548\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4184 - loss: 0.3790 - val_accuracy: 0.7465 - val_loss: 1.3884\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3475 - loss: 0.4142 - val_accuracy: 0.7465 - val_loss: 1.4259\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3309 - loss: 0.4555 - val_accuracy: 0.7592 - val_loss: 1.3847\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2505 - loss: 0.5670 - val_accuracy: 0.7592 - val_loss: 1.3823\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2937 - loss: 0.4457 - val_accuracy: 0.7592 - val_loss: 1.4056\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2473 - loss: 0.5339 - val_accuracy: 0.7592 - val_loss: 1.4030\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3789 - loss: 0.3430 - val_accuracy: 0.7592 - val_loss: 1.4144\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2718 - loss: 0.4536 - val_accuracy: 0.7592 - val_loss: 1.4155\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3192 - loss: 0.3645 - val_accuracy: 0.7592 - val_loss: 1.4410\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2569 - loss: 0.5257 - val_accuracy: 0.7465 - val_loss: 1.4324\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3253 - loss: 0.3590 - val_accuracy: 0.7465 - val_loss: 1.4424\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2866 - loss: 0.5089 - val_accuracy: 0.7465 - val_loss: 1.4768\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3452 - loss: 0.3874 - val_accuracy: 0.7465 - val_loss: 1.4401\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2488 - loss: 0.4894 - val_accuracy: 0.7465 - val_loss: 1.4413\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3531 - loss: 0.3404 - val_accuracy: 0.7465 - val_loss: 1.5104\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2562 - loss: 0.4833 - val_accuracy: 0.7465 - val_loss: 1.4460\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.3716 - loss: 0.3568 - val_accuracy: 0.7465 - val_loss: 1.4549\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2369 - loss: 0.4984 - val_accuracy: 0.7465 - val_loss: 1.4646\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3982 - loss: 0.3076 - val_accuracy: 0.7592 - val_loss: 1.4527\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3360 - loss: 0.3717 - val_accuracy: 0.7465 - val_loss: 1.5069\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2859 - loss: 0.3862 - val_accuracy: 0.7465 - val_loss: 1.4593\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4032 - loss: 0.2877 - val_accuracy: 0.7465 - val_loss: 1.4669\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2980 - loss: 0.3833 - val_accuracy: 0.7592 - val_loss: 1.4762\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2512 - loss: 0.4021 - val_accuracy: 0.7465 - val_loss: 1.4854\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3397 - loss: 0.3081 - val_accuracy: 0.7465 - val_loss: 1.5201\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2634 - loss: 0.4347 - val_accuracy: 0.7465 - val_loss: 1.4795\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2547 - loss: 0.4444 - val_accuracy: 0.7592 - val_loss: 1.4468\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3073 - loss: 0.3318 - val_accuracy: 0.7465 - val_loss: 1.5189\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4496 - loss: 0.2651 - val_accuracy: 0.7592 - val_loss: 1.4947\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2498 - loss: 0.4012 - val_accuracy: 0.7592 - val_loss: 1.4840\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3925 - loss: 0.2553 - val_accuracy: 0.7465 - val_loss: 1.5145\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.3650 - loss: 0.2768 - val_accuracy: 0.7465 - val_loss: 1.5103\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2194 - loss: 0.4420 - val_accuracy: 0.7592 - val_loss: 1.5332\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2404 - loss: 0.4157 - val_accuracy: 0.7592 - val_loss: 1.4697\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2490 - loss: 0.4452 - val_accuracy: 0.7592 - val_loss: 1.5171\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3607 - loss: 0.2607 - val_accuracy: 0.7465 - val_loss: 1.5464\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3797 - loss: 0.2520 - val_accuracy: 0.7592 - val_loss: 1.5291\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.3511 - loss: 0.3114 - val_accuracy: 0.7592 - val_loss: 1.5349\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3171 - loss: 0.2733 - val_accuracy: 0.7592 - val_loss: 1.5086\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3475 - loss: 0.2678 - val_accuracy: 0.7592 - val_loss: 1.5358\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3638 - loss: 0.2758 - val_accuracy: 0.7592 - val_loss: 1.5168\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2755 - loss: 0.3421 - val_accuracy: 0.7592 - val_loss: 1.5014\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2355 - loss: 0.3461 - val_accuracy: 0.7465 - val_loss: 1.6025\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Токенизированная последовательность: [[]]\n",
      "Предсказанные индексы: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Перевод: \n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Преобразование выходных данных для обучения\n",
    "padded_tat = np.expand_dims(padded_tat, -1)\n",
    "\n",
    "# Увеличение количества эпох обучения\n",
    "model.fit(padded_eng, padded_tat, epochs=100, batch_size=2, validation_split=0.2)\n",
    "\n",
    "# Функция для перевода текста\n",
    "def translate_sentence(sentence):\n",
    "    # Токенизация и паддинг ввода\n",
    "    sequence = tokenizer_eng.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Предсказание\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    predicted_sentence = np.argmax(prediction, axis=-1)\n",
    "    \n",
    "    # Преобразование предсказанных индексов в слова, убираем '[UNK]'\n",
    "    translated_sentence = ' '.join(tokenizer_tat.index_word.get(i) for i in predicted_sentence[0] if i != 0)\n",
    "    print('Токенизированная последовательность:', sequence)\n",
    "    print('Предсказанные индексы:', predicted_sentence[0])\n",
    "    return translated_sentence\n",
    "\n",
    "# Тест функции перевода\n",
    "user_sentence = \"The media routinely talk about a historic turn, but people who have been involved in Korean affairs for decades do not share this optimism.\"\n",
    "translated_sentence = translate_sentence(user_sentence)\n",
    "print('Перевод:', translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "шулай ук җинаятьләрне мәктәптән өйгә кайтканда уйлап табалар\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence):\n",
    "    # Токенизация и паддинг ввода\n",
    "    sequence = tokenizer_eng.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Предсказание\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    predicted_sentence = np.argmax(prediction, axis=-1)\n",
    "    \n",
    "    # Преобразование предсказанных индексов в слова, убираем '[UNK]'\n",
    "    translated_sentence = ' '.join(tokenizer_tat.index_word.get(i) for i in predicted_sentence[0] if i != 0)\n",
    "    return translated_sentence\n",
    "\n",
    "# Тест функции перевода\n",
    "user_sentence = \"Также преступления замышляются по дороге из школы домой.\"\n",
    "translated_sentence = translate_sentence(user_sentence)\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('translation_model.keras')\n",
    "\n",
    "import pickle\n",
    "with open('tokenizer_eng.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_eng, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('tokenizer_tat.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_tat, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\работа\\прога\\ai\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 643ms/step - accuracy: 0.3805 - loss: 4.7145 - val_accuracy: 0.9245 - val_loss: 3.0002\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7270 - loss: 2.9302 - val_accuracy: 0.9245 - val_loss: 0.4752\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7179 - loss: 1.8170 - val_accuracy: 0.9245 - val_loss: 0.4525\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.6987 - loss: 1.6637 - val_accuracy: 0.9245 - val_loss: 0.5271\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6903 - loss: 1.6620 - val_accuracy: 0.9245 - val_loss: 0.5716\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7044 - loss: 1.6218 - val_accuracy: 0.9245 - val_loss: 0.5202\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.6528 - loss: 1.7945 - val_accuracy: 0.9245 - val_loss: 0.4825\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6481 - loss: 1.8363 - val_accuracy: 0.9245 - val_loss: 0.4813\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7179 - loss: 1.4476 - val_accuracy: 0.9245 - val_loss: 0.4921\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7123 - loss: 1.4372 - val_accuracy: 0.9245 - val_loss: 0.5096\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6701 - loss: 1.6500 - val_accuracy: 0.9245 - val_loss: 0.5141\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7390 - loss: 1.2848 - val_accuracy: 0.9245 - val_loss: 0.5087\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6632 - loss: 1.6299 - val_accuracy: 0.9245 - val_loss: 0.5071\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6484 - loss: 1.6758 - val_accuracy: 0.9245 - val_loss: 0.5125\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7157 - loss: 1.3613 - val_accuracy: 0.9245 - val_loss: 0.5211\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6601 - loss: 1.6041 - val_accuracy: 0.9245 - val_loss: 0.5305\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6931 - loss: 1.4277 - val_accuracy: 0.9245 - val_loss: 0.5357\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7384 - loss: 1.1870 - val_accuracy: 0.9245 - val_loss: 0.5429\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6903 - loss: 1.4001 - val_accuracy: 0.9245 - val_loss: 0.5537\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6698 - loss: 1.4791 - val_accuracy: 0.9245 - val_loss: 0.5704\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7314 - loss: 1.2202 - val_accuracy: 0.9245 - val_loss: 0.5792\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6818 - loss: 1.3899 - val_accuracy: 0.9245 - val_loss: 0.5917\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6497 - loss: 1.4809 - val_accuracy: 0.9245 - val_loss: 0.6085\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6865 - loss: 1.3109 - val_accuracy: 0.9245 - val_loss: 0.6241\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6824 - loss: 1.2661 - val_accuracy: 0.9245 - val_loss: 0.6406\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7384 - loss: 1.0552 - val_accuracy: 0.9245 - val_loss: 0.6390\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7425 - loss: 1.0625 - val_accuracy: 0.9245 - val_loss: 0.6327\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7550 - loss: 0.9538 - val_accuracy: 0.9245 - val_loss: 0.6276\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7214 - loss: 1.0548 - val_accuracy: 0.9245 - val_loss: 0.6510\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.7635 - loss: 0.8665 - val_accuracy: 0.9245 - val_loss: 0.6622\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6843 - loss: 1.1340 - val_accuracy: 0.9245 - val_loss: 0.6801\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7664 - loss: 0.8972 - val_accuracy: 0.9245 - val_loss: 0.6625\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7230 - loss: 0.9257 - val_accuracy: 0.9151 - val_loss: 0.7041\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7283 - loss: 0.9513 - val_accuracy: 0.9245 - val_loss: 0.6739\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7698 - loss: 0.8368 - val_accuracy: 0.9245 - val_loss: 0.6904\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7365 - loss: 0.9412 - val_accuracy: 0.9245 - val_loss: 0.7015\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7682 - loss: 0.7859 - val_accuracy: 0.9245 - val_loss: 0.6974\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8041 - loss: 0.6877 - val_accuracy: 0.9245 - val_loss: 0.6836\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7528 - loss: 0.8811 - val_accuracy: 0.9245 - val_loss: 0.6815\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7701 - loss: 0.7500 - val_accuracy: 0.9057 - val_loss: 0.7544\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6780 - loss: 0.9966 - val_accuracy: 0.9245 - val_loss: 0.7142\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7780 - loss: 0.7843 - val_accuracy: 0.9245 - val_loss: 0.7088\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8292 - loss: 0.5971 - val_accuracy: 0.9151 - val_loss: 0.7331\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7912 - loss: 0.7419 - val_accuracy: 0.9151 - val_loss: 0.7186\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7972 - loss: 0.7284 - val_accuracy: 0.9151 - val_loss: 0.7164\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7623 - loss: 0.7869 - val_accuracy: 0.9151 - val_loss: 0.7399\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8336 - loss: 0.6318 - val_accuracy: 0.9245 - val_loss: 0.7343\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8311 - loss: 0.5730 - val_accuracy: 0.9245 - val_loss: 0.7269\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7651 - loss: 0.8062 - val_accuracy: 0.9151 - val_loss: 0.7574\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7925 - loss: 0.7435 - val_accuracy: 0.9245 - val_loss: 0.7623\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7871 - loss: 0.8153 - val_accuracy: 0.9151 - val_loss: 0.7722\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8003 - loss: 0.7619 - val_accuracy: 0.9151 - val_loss: 0.7735\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8101 - loss: 0.6560 - val_accuracy: 0.9151 - val_loss: 0.7827\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8327 - loss: 0.6385 - val_accuracy: 0.9151 - val_loss: 0.7909\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8925 - loss: 0.4607 - val_accuracy: 0.9151 - val_loss: 0.8050\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8745 - loss: 0.4971 - val_accuracy: 0.9151 - val_loss: 0.7970\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8585 - loss: 0.5417 - val_accuracy: 0.9151 - val_loss: 0.8021\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8541 - loss: 0.5269 - val_accuracy: 0.9151 - val_loss: 0.8219\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8824 - loss: 0.4540 - val_accuracy: 0.9151 - val_loss: 0.8046\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7827 - loss: 0.6281 - val_accuracy: 0.9151 - val_loss: 0.8405\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7786 - loss: 0.6375 - val_accuracy: 0.9151 - val_loss: 0.8223\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8299 - loss: 0.4613 - val_accuracy: 0.9151 - val_loss: 0.8479\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7274 - loss: 0.6417 - val_accuracy: 0.9151 - val_loss: 0.8411\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8673 - loss: 0.4790 - val_accuracy: 0.9151 - val_loss: 0.8359\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8469 - loss: 0.5048 - val_accuracy: 0.9151 - val_loss: 0.8611\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8509 - loss: 0.3875 - val_accuracy: 0.9151 - val_loss: 0.8381\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8704 - loss: 0.4073 - val_accuracy: 0.9151 - val_loss: 0.8448\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9308 - loss: 0.3697 - val_accuracy: 0.9057 - val_loss: 0.8667\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9063 - loss: 0.3765 - val_accuracy: 0.9151 - val_loss: 0.8381\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8745 - loss: 0.4915 - val_accuracy: 0.9151 - val_loss: 0.8628\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8972 - loss: 0.3998 - val_accuracy: 0.9151 - val_loss: 0.8694\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8991 - loss: 0.3884 - val_accuracy: 0.9151 - val_loss: 0.8666\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9151 - loss: 0.3652 - val_accuracy: 0.9151 - val_loss: 0.8895\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9255 - loss: 0.3491 - val_accuracy: 0.9151 - val_loss: 0.8771\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8840 - loss: 0.3813 - val_accuracy: 0.9151 - val_loss: 0.8992\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9292 - loss: 0.3080 - val_accuracy: 0.9151 - val_loss: 0.8978\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8912 - loss: 0.4476 - val_accuracy: 0.9151 - val_loss: 0.9260\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8881 - loss: 0.4059 - val_accuracy: 0.9151 - val_loss: 0.9142\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9138 - loss: 0.4101 - val_accuracy: 0.9151 - val_loss: 0.9179\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9418 - loss: 0.2773 - val_accuracy: 0.9151 - val_loss: 0.8992\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9264 - loss: 0.2882 - val_accuracy: 0.9151 - val_loss: 0.9190\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8638 - loss: 0.4419 - val_accuracy: 0.9151 - val_loss: 0.9090\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9050 - loss: 0.3324 - val_accuracy: 0.9151 - val_loss: 0.9184\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9069 - loss: 0.3218 - val_accuracy: 0.9057 - val_loss: 0.9462\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8921 - loss: 0.3465 - val_accuracy: 0.9151 - val_loss: 0.9124\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8934 - loss: 0.3482 - val_accuracy: 0.9057 - val_loss: 0.9355\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8025 - loss: 0.5237 - val_accuracy: 0.9151 - val_loss: 0.9103\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9148 - loss: 0.2625 - val_accuracy: 0.9151 - val_loss: 0.9007\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9097 - loss: 0.3268 - val_accuracy: 0.9151 - val_loss: 0.9401\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9387 - loss: 0.2833 - val_accuracy: 0.9151 - val_loss: 0.9380\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9110 - loss: 0.3352 - val_accuracy: 0.9057 - val_loss: 0.9616\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8833 - loss: 0.4541 - val_accuracy: 0.9057 - val_loss: 0.9615\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8991 - loss: 0.2757 - val_accuracy: 0.9151 - val_loss: 0.9282\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9035 - loss: 0.3387 - val_accuracy: 0.9057 - val_loss: 0.9545\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9487 - loss: 0.2878 - val_accuracy: 0.9057 - val_loss: 0.9572\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9393 - loss: 0.2871 - val_accuracy: 0.9151 - val_loss: 0.9278\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8406 - loss: 0.4067 - val_accuracy: 0.9151 - val_loss: 0.9436\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8846 - loss: 0.4155 - val_accuracy: 0.9151 - val_loss: 0.9436\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9223 - loss: 0.2873 - val_accuracy: 0.9057 - val_loss: 0.9636\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9374 - loss: 0.3031 - val_accuracy: 0.9151 - val_loss: 0.9584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, TimeDistributed, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "# Представим, что у нас есть набор данных с предложениями\n",
    "english_sentences = pd.read_csv(\"concatenated_dataset.csv\")['en'][:10]\n",
    "\n",
    "tatar_sentences = pd.read_csv(\"concatenated_dataset.csv\")['tat'][:10]\n",
    "\n",
    "# Токенизация текста\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_tat = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(english_sentences)\n",
    "tokenizer_tat.fit_on_texts(tatar_sentences)\n",
    "\n",
    "# Преобразование текста в последовательности чисел\n",
    "sequences_eng = tokenizer_eng.texts_to_sequences(english_sentences)\n",
    "sequences_tat = tokenizer_tat.texts_to_sequences(tatar_sentences)\n",
    "\n",
    "# Паддинг последовательностей для выравнивания длины\n",
    "max_len_eng = max(len(x) for x in sequences_eng)\n",
    "max_len_tat = max(len(x) for x in sequences_tat)\n",
    "max_len = max(max_len_eng, max_len_tat)  # Выровнять длину последовательностей по максимальному значению\n",
    "\n",
    "padded_eng = pad_sequences(sequences_eng, maxlen=max_len, padding='post')\n",
    "padded_tat = pad_sequences(sequences_tat, maxlen=max_len, padding='post')\n",
    "\n",
    "# Параметры модели\n",
    "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
    "vocab_size_tat = len(tokenizer_tat.word_index) + 1\n",
    "embedding_dim = 128\n",
    "units = 128\n",
    "# Построение модели\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size_eng, embedding_dim, input_length=max_len),\n",
    "    Bidirectional(LSTM(units, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(units, return_sequences=True)),\n",
    "    TimeDistributed(Dense(vocab_size_tat, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Преобразование выходных данных для обучения\n",
    "padded_tat = np.expand_dims(padded_tat, -1)\n",
    "\n",
    "# Увеличение количества эпох обучения\n",
    "model.fit(padded_eng, padded_tat, epochs=100, batch_size=2, validation_split=0.2)\n",
    "\n",
    "# Сохранение модели и токенизаторов\n",
    "model.save('translation_model.h5')\n",
    "\n",
    "import pickle\n",
    "with open('tokenizer_eng.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_eng, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('tokenizer_tat.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_tat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\работа\\прога\\ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Загрузка модели и токенизаторов\n",
    "model = load_model('translation_model.h5')\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_tat = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(english_sentences)\n",
    "tokenizer_tat.fit_on_texts(tatar_sentences)\n",
    "\n",
    "# Оптимизация функции предсказания\n",
    "@tf.function\n",
    "def fast_predict(model, input_sequence):\n",
    "    return model(input_sequence)\n",
    "\n",
    "# Функция для перевода текста\n",
    "def translate_sentence(sentence):\n",
    "    # Токенизация и паддинг ввода\n",
    "    sequence = tokenizer_eng.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Предсказание\n",
    "    prediction = fast_predict(model, padded_sequence)\n",
    "    predicted_sentence = np.argmax(prediction, axis=-1)\n",
    "    \n",
    "    # Преобразование предсказанных индексов в слова, убираем '[UNK]'\n",
    "    translated_sentence = ' '.join(tokenizer_tat.index_word.get(i, '') for i in predicted_sentence[0] if i != 0)\n",
    "    return translated_sentence\n",
    "\n",
    "# Создание приложения Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/translate', methods=['POST'])\n",
    "def translate():\n",
    "    data = request.get_json()\n",
    "    sentence = data.get('sentence')\n",
    "    if not sentence:\n",
    "        return jsonify({'error': 'No sentence provided'}), 400\n",
    "    \n",
    "    translated_sentence = translate_sentence(sentence)\n",
    "    return jsonify({'translated_sentence': translated_sentence})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error retrieving books by genre: 500\n"
     ]
    }
   ],
   "source": [
    "# Задайте URL для получения списка книг по жанру\n",
    "url_genre = \"http://127.0.0.1:5000/books/by_genre/Fiction\"  # Замените 'Fiction' на нужный жанр\n",
    "\n",
    "# Отправьте GET запрос\n",
    "response_genre = requests.get(url_genre)\n",
    "\n",
    "# Проверьте статус ответа и выведите результат\n",
    "if response_genre.status_code == 200:\n",
    "    print(\"Books in genre:\")\n",
    "    print(response_genre.json())\n",
    "else:\n",
    "    print(\"Error retrieving books by genre:\", response_genre.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "очрашу очрашу белән\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizers\n",
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "model = load_model('translation_model.h5')\n",
    "with open('tokenizer_eng.pkl', 'rb') as handle:\n",
    "    tokenizer_eng = pickle.load(handle)\n",
    "with open('tokenizer_tat.pkl', 'rb') as handle:\n",
    "    tokenizer_tat = pickle.load(handle)\n",
    "\n",
    "def translate_sentence(sentence):\n",
    "    # Токенизация и паддинг ввода\n",
    "    sequence = tokenizer_eng.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, padding='post')\n",
    "    \n",
    "    # Предсказание\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    predicted_sentence = np.argmax(prediction, axis=-1)\n",
    "    \n",
    "    # Преобразование предсказанных индексов в слова, убираем '[UNK]'\n",
    "    translated_sentence = ' '.join(tokenizer_tat.index_word.get(i) for i in predicted_sentence[0] if i != 0)\n",
    "    return translated_sentence\n",
    "\n",
    "# Тест функции перевода\n",
    "user_sentence = \"Также преступления замышляются по дороге из школы домой.\"\n",
    "translated_sentence = translate_sentence(user_sentence)\n",
    "print(translated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
